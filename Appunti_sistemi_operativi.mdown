# Appunti Sistemi Operativi
<link rel="stylesheet" type="text/css" href="css/custom.css">
## Processi 

Un processo è un istanza di un programma in esecuzione.
Ogni processo ha un immagine di memoria, la quale è composta dai seguenti campi:

1. Codice o testo: contiene il codice vero e proprio che deve essere eseguito;
2. Dati: contiene l'insieme delle variabili globali. (è la sezione data dell'asm);
3. Stack: il normalissimo stack di esecuzione di un processo;
4. Heap: parte della memoria dinamica di un processo;
5. Attributi: è un campo che contiene un certo numero di informazioni. relative al processo, utili al sistema operativo. I più importanti di questi sono il PCB ed il PID (Process ID);

### PCB

Il PCB (Process Control Block) è un attributo che viene associato ad ogni processo. Contiene una serie di campi che sono necessari alle operazioni di Scheduling e di Dispatching. Questi campi sono: 

* Pointer: ...;
* Program State: è lo stato del processo, può essere: pronto, in esecuzione, in attesa o in swap;
* Program number: è il numero relativo al processo;
* Program counter: è il valore del registro program counter il quale è necessario per il cambio di contesto.
* Registers: è una zona di memoria che contiene il contenuto dei registri nel momento in cui il processo è stato interrotto l'ultima volta. Questo campo come il precedente è utile nelle operazioni di cambio di contesto.
* Memory limits: Come è ben risaputo ogni processo all'interno del OS ha bisogno del proprio spazio di memoria e non può scrivere o leggere nello spazion di memoria associatio ad un altro processo. Questo campo identifica i limiti di memoria in cui il processo **deve** stare.
* List Of Open Files: Il nome è auto esplicativo, contiene la lista dei file aperti. 

### Stati di un processo

Ogni processo all'interno di un sistema operativo è in uno _stato_. Supponiamo di avere un processo che debba effettuare delle operazioni di input output, fintanto che queste operazioni non sono terminate è inutile tentare di rieseguire il processo dato che il processo non potrebbe fare nulla se non occupare la CPU inutilmente.

Gli stati possibili di un processo sono 3 (4 se contiamo lo swap ma si parlerà di questo quando si parlerà di Scheduling):

1. Pronto: Il processo è già stato ammesso nel sistema ed è pronto per essere eseguito.
2. Esecuzione: Il processo sta effettivamente utilizzando la CPU. Ovviamente solo un processo alla volta per CPU può trovarsi contemporaneamente in questo stato.
3. Attesa: Un processo si può trovare in questo stato quando sta attendendo un evento, che questo sia di I/O oppure che venga scatenato da un altro processo.

### Operazioni sui processi

#### Scheduling

L'operazione di scheduling è un operazione che permette di decidere quale tra i vari processi nello stato di pronto debba essere mandato in esecuzione. Questa operazione **NON** effettua il cambio di contesto. Il processo del OS che si occupa di effettuare lo Scheduling è lo Scheduler.

#### Dispatching

Questa è l'operazione che fisicamente effettua il cambio di contesto. Viene effettuata dal Dispatcher ed è composta dai seguenti passaggi: 

1. Salvare lo stato del processo vecchio nel suo PCB. Quindi salvare tutti i registri ed il program counter.
2. Prelevare i dati dei vari registri compreso il Program counter dal PCB del nuovo processo e metterli nei rispettivi registri.
3. Passare dalla modalità supervisore alla modalità utente per dare il controllo della CPU al processo nuovo.

### Operazioni dei processi

Ogni processo può effettuare determinate operazioni riguardanti altri processi.

#### Creazione di un processo

Ogni processo può creare dei processi detti processi figli. Questa procedura di creazione del processo si può svolgere in due modi: il primo è quello spartire le risorse con il figlio e quindi di non richiedere risorse aggiuntive all'OS, il secondo è quello di creare un processo che richieda risorse al Sistema esattamente come se fosse un processo nuovo.

Oltre che nel modo di creazione vi è un altra importante distinzione tra i processi figli: **La modalità di esecuzione**. La quale può essere:

* Sincrona: il processo padre si blocca ed attende che il processo figlio abbia terminato un determinato compito;
* Asincrona: il processo padre ed il figlio procedono in modo _parallelo_.

##### Creazione di un processo sotto linux

Sotto linux è possibile creare un processo nel seguente modo.

```c
#include <stdio.h>
void main(int argc, char *argv[]){

    int pid;
    pid = fork(); /* genera un nuovo processo */

    if (pid < 0) { /* errore */
        fprintf(stderr, “Errore di creazione”); 
        exit(-1);
    } else if (pid == 0) { /* codice del figlio */
        execlp(“/bin/ls”, “ls”, NULL); } 
    else { /* codice del padre */
        wait(NULL); /* padre attende il figlio */
        printf(“Figlio ha terminato.”);
        exit(0); 
    }
}
```

Le tre funzioni importanti sono:

* Fork: questa è un system call che genera un processo **Identico** al padre in tutto e per tutto. Restituisce: un numero negativo in caso di errore, 0 al processo figlio ed il pid del figlio al processo padre (ricorda che questa funizone viene chiamata in entrambi i processi).
* Excec: questa System Call si occupa di **sostituire** il codice del processo con quello di un altro processo specificato come parametro.
* Wait: permette l'esecuzione sincrona, cioè permette al processo di attendere la terminazione di uno dei suoi processi figli.

#### Terminazione di un processo

Un processo può terminare per tre ragioni:

1. Il processo finisce la sa esecuzione;
2. Il processo viene terminato forzatamente dal padre. Può accadere per le più svariate ragioni;
3. Il processo viene terminato forzatamente dal OS a causa di un errrore.

## Thread

Un thread è definibile come un unità minima di esecuzione della CPU.
Ogni thread è collegato ad un processo e di questo condivide: Lo spazio di indirizzamento e le risorse del sistema.

Ciò che cambia tra un thread ed un altro è:

1. Stato di esecuzione;
2. Program counter;
3. Stack;
4. Insieme dei registri;

Per far si che un processo possa eseguire più di un thread alla volta è necessario che l'OS sul quale gira supporti il **Multithreading** 

### Vantaggi nell'utilizzo delle thread

Vi sono numerosi vantaggi nell'utilizzo delle thread al posto dei processi: 

* Riduzione del tempo di risposta: Dato che una thread può essere eseguita mentre un altra sta effettuando delle operazioni bloccanti (es. I/O);
* Condivisione delle risorse: Le thread condividono lo spazio di indirizzamento per cui non si rendono necessari tutti i meccanismi di comunicazioni che sono necessari per la comunicazione tra processi;
* Il context switch, la creazione e la terminazione dei processi sono molto più lente rispetto alle corrispettive operazioni per le thread.
* Se le thread vengono fatte girare su un sitema multiprocessore aumentano il parallelismo del processo che può far svolgere un thread in ogni processore.

### Tipi di implementazione delle thread

Per implementare le thread vi sono due metodi fondamentali:

1. User-level thread: Sono delle thread che vengono implementate a livello utente tramite delle normali librerie. Il kernel in questo modo ignora l'esistenza delle thread le quali vengono parallelizzate nel quanto di tempo dedicato al processo (es. le thread di java).
    * Vantaggi: 
        - Non rendono necessario il passaggio alla modalità kernel per eseguire il cambio di contesto tra thread il che aumenta l'efficienza
        - Il meccanismo di scheduling può variare da applicazione ad applicazione basta cambiare l'implementazione.
        - Portabilità del codice: il codice dell'applicazione non è legato alle system call del kernel per eseguire le thread.
    * Svantaggi:
        - Il blocco di una thread blocca l'intero processo;
        - non è possibile sfruttare un multiprocessore, dato che il kernel non sa che il processore sta eseguendo più thread. 
2. Kernel-level thread: Queste al contrario delle prime vengono create dal kernel il quale ne conosce l'esistenza e le schedula come ritiene più opportuno (es. le PThread).
    * Vantaggi:
        - Il blocco di un thread non blocca l'intero processo: è infatti il kernel a schedulare ogni thread separatamente.
        - Più thread possono essere attivi allo stesso momento su diverse CPU
        - anche i processi del OS possono essere multithreading.
    * Svantaggi:
        - Scarsa efficienza: ogni context switch deve passare per il kernel.
3. Esistono anche degli approcci che combinano i due precedenti modelli (es. sun solaris).

![Esempio implementazione thread](res/thread.png)

## Esecuzione del Sistema Operativo

Il sistema operativo può essere visto a tutti gli effetti come un insieme di processi. Quindi si pongono dei quesiti abbastanza importanti:

* Quando viene eseguito l'OS? 
* In che modo viene eseguito l'OS? 

Per rispondere alla prima domanda: il sistema operativo viene schedulato esattamente come gli altri processi, ma oltre a ciò questo prende il controllo della macchina anche per rispondere a determinati interrupt che vengono generati a livello hardware.

Per quanto riguarda la seconda domanda la risposta dipenda da come viene implementato l'OS. Vi sono 3 possibilità: 

1. Kernel separato: In questa implementazione il server agisce al di fuori di ogni processo e **NON** viene considerato come un processo qualunque. Questo infatti esegue in una modalità privilegiata e possiede uno spazio di indirizzamento privato in memoria.
2. Kernel eseguito all'interno di un processo utente: In questa versione viene creata una serie di funzioni (System Call) che possono essere eseguite in modalità protetta ma che vengono chiamate dai normalissimi processi utente. Per fare questo si fa in modo che il codice ed i dati del sistema operativo siano condivisi tra tutti i processi utente in modo che possano accedere senza problemi alle funzioni necessarie.
Questa implementazione ha dei vantaggi che vanno considerati:
    * Quando arriva un interrupt non è necessario effettuare un Context switch ma è sufficente effettuare un *mode* switch il quale consiste solo nel cambio di modalità e permette di evitare la perdita di tempo relativa al context switch.
    * L'OS può decidere, al termine del suo lavoro, se effettuare un altro mode switch oppure se effettuare un Context Switch e cambiare processo in esecuzione.
3. Kernel visto come insieme di processi effettivi: In questo caso il kernel viene eseguito in modalità protetta, ma come un insieme di processi di conseguenza questi processi vengono schedulati uno alla volta esattamente come gli altri. Ovviamente una piccola parte del sistema deve essere al di sopra dei processi (almeno lo scheduler). I vantaggi di questa implementazione sono:
    * La mantenibilità e la modularità del codice: cambiare un processo del sistema operativo risulta molto semplice;
    * In sistemi multiprocessore è possibile che uno o più processori siano costantemente dedicati ai processi del sistema operativo. 


## Scheduling

Lo scheduling di un processo significa assegnargli una quantità di tempo ben definita. Per poter eseguire un processo con il paradigma della multiprogrammazione è necessario seguire delle regole ben definite sia per l'ammissione di un processo nel sistema (quindi in memoria) sia per l'esecuzione di tale processo da parte della CPU.

Un processo che deve essere eseguito deve quindi essere messo in una coda dei processi pronti mentre uno che deve effettuare degli I/O deve uscire terminare gli I/O e successivamente essere rimesso nella coda dei processi pronti.
Un possibile diagramma di accodamento è il seguente:

![diagramma di accodamento](res/diagramma_accodamento.png)

### Tipi di scheduler

Esistono vari tipi di scheduler che si occupano di cose differenti:

* Scheduler a breve termine: si occupa di schedulare accesso di processi alla CPU. Il compito di questo scheduler è molto importante e va fatto in meno tempo possibile dato che viene eseguito molto spesso ( es 1 volta ogni millisecondo ) quindi è necessario che sia basato su un algoritmo che sia O(&#xb5;s). 
* Scheduler a lungo termine: si occupa dello scheduling dei processi in memoria, serve per l'ammissione dei processi al sistema e quindi viene chiamato molto più raramente di quello a breve termine. Ci si accontenta di un algoritmo in O(ms).
* Scheduler a medio termine: si tratta di uno scheduler che viene chiamato solamente quando si hanno sistemi con memoria virtuale. Ai occupa di strappare momentaneamente un processo dalla CPU e portarlo nella memoria fisica allo scopo di ridurre il grado di multiprogrammazione dell'insieme di processi.

![diagramma scheduler](res/diagramma_schedulers.png)

### Scheduling della CPU

Lo scheduling della CPU è una parte molto critica del sistema operativo, a causa della sua grande frequenza di invocazione.

#### Burst

Lo schema di esecuzione di un processo si basa sul concetto di _**Burst**_ il quale è una sequenza di operazioni: ci possono essere burst di CPU oppure burst di I/O.
L'esecuzione altro non è che l'alternanza ciclica di burst di CPU e burst di I/O.   

#### Preemption

Un altro meccanismo fondamentale nella trattazione degli OS è la prelazione (preemption). Questo meccanismo è quello che permette al sistema operativo di costringere un processo a rilasciare la CPU nonostante questo non abbia ancora finito il suo burst di esecuzione. Questo è un concetto che non è sempre presente all'interno di tutti gli algoritmi di scheduling, ma che per alcuni risulta essenziale (es. round robin).


### Algoritmi di scheduling

Esistono vari algoritmi di scheduling. Ognuno dei quali ha sia alcuni vantaggi che alcuni svantaggi, Si rende quindi necessario avere delle metriche per determinare quale tra i vari algoritmi è il migliore in un caso o nell'altro. Le metriche più utilizzate sono le seguenti:

* Utilizzo della CPU: si vuole ottenere il massimo utilizzo possibile della CPU evitando momenti in cui questa si trovi in idle o in polling ("a non far niente").
* Throughput: Questo indica il numero di processi completati nell'unità di tempo. Nonostante questo parametro sia legato a quello precedente (difficilmente si può avere un throughput elevato con un utilizzo basso di CPU oppure il contrario) questi non sono la stessa cosa.
* Tempo di attesa: Indica la quantità totale di tempo sprecato da un processo in coda di attesa. Ovviamente più alto è il valore peggiore si rivelerà essere l'algoritmo. 
* Turnaround (tempo di completamento): indica il tempo totale impiegato da un processo per terminare da quando viene ammesso nel sistema. Questo parametro diventa veramente importante se messo in relazione con il tempo di risposta ed il tempo di attesa.
* Response time (Tempo di risposta): indica il tempo trascorso dall'ammissione di un processo nel sistema fino alla sua prima esecuzione. 

Un algoritmo di scheduling è buono quanto più riesce a:

1. Minimizzare l'utilizzo della CPU ed il Throughput.
2. Massimizzare i tempi di turnaround, attesa e risposta.

#### First Come First Served (FCFS)

È uno degli algoritmi di scheduling più semplici che si possano implementare. Si potrebbe dire che il paradigma utilizzato è: "Chi prima arriva meglio alloggia", infatti la coda dei processi viene tratta come una coda FIFO (first in first out). 

Come detto precedentemente questo algoritmo è molto semplice da implementare ma ovviamente risulta non essere molto efficiente soprattutto perché non prevede alcun tipo di prelazione, il che causa un **effetto convoglio**.
L'effetto convoglio è un termine per indicare l'accodamento di processi brevi dietro a processi lunghi precedentemente arrivati, questo abbassa drasticamente il throughput il che ovviamente rende questo algoritmo meno appetibile.

**N.B.** L'effetto convoglio è un classico esempio della differenza tra Throughput ed Utilizzo della CPU (vedesi img).

![Esempio effetto convoglio](res/es_effetto_convoglio.png)

#### Shortest-Job-First (SJF)

Questo algoritmo nasce dalla necessità di evitare l'effetto convoglio. Funziona nel modo seguente:

1. Associa ad ogni processo una lunghezza del prossimo burst di CPU, stimandola mediante una formula che si vedrà in seguito.
2. Il processo con il burst più breve viene selezionato.
3. Alla fine del burst di CPU del processo selezionato la procedura viene ripetuta.

Questo algoritmo per sua definizione non è necessariamente preemtive, ma ne esiste una versione che ammette prelazione chiamata **Short-Remaining-Time-First** che funziona nel modo seguente:

1. Associa ad ogni processo una lunghezza del prossimo burst di CPU, stimandola mediante una formula che si vedrà in seguito.
2. Il processo con il burst più breve viene selezionato.
3. Ad ogni arrivo di un nuovo processo nella ready queque la procedura viene ripetuta.

**N.B.** SJF è un algoritmo ottimo rispetto all'ottimizzazione del tempo di attesa!

##### Stima del successivo burst di CPU

Come scritto in precedenza è possibile effettuare solo una stima del prossimo burst di CPU. Questa viene fatta utilizzando una formula che tiene conto di due fattori: 

1. La lunghezza del burst precedente t<sub>n</sub>.
2. la stima fatta la volta precedente &tau;<sub>n</sub>.

Questi due fattori vengono utilizzati insieme ad un parametro che viene scelto in fase implementativa: 0 &le; &alpha; &le; 1.

La formula per la stima è la seguente:
<center>τ<sub>n+1</sub> = α * t<sub>n</sub> + (1-α) * τ<sub>n</sub></center>

**N.B.** Ci sono un paio di osservazioni importanti:

* Cambiando il valore di &alpha; si sta in effetti decidendo quanto tener conto delle lunghezze dei burst avuti in passato e quanto invece tener conto del burst immediatamente precedente. Infatti:
    - &alpha; = 0 &rarr; &tau;<sub>n+1</sub> = &tau;<sub>n</sub>
    - &alpha; = 1 &rarr; &tau;<sub>n+1</sub> = t<sub>n</sub>
* Espandendo la formula (è ricorsiva) si nota immediatamente che scegliendo &alpha; &le; 1 si fa in modo che ogni termine successivo pesa meno di quello precedente. Questo implica che si tende ad ignorare la storia "vecchia" ed a tenere in maggior considerazione la storia "recente".

#### Scheduling a priorità 

L'SJF è un algoritmo che in qualche modo contiene il concetto di priorità, basti pensare alla priorità come se fosse: <span class="fr"><span>1</span><span class="fd">burst<sub>CPU</sub></span></span>.

Il procedimento di un generico algoritmo di scheduling con priorità è il seguente:

1. Ad ogni processo viene associata una priorità.
2. Viene messo in esecuzione il processo a priorità più alta.
3. si ripete dal punto 1.

Le problematiche che subito vengono in mente in questo caso sono le seguenti:

* Come assengare la priorità.
* Scegliere se si vuole implementare un algoritmo preemtive o meno.
* Se continuano a venire fuori degli algoritmi con priorità molto alta, uno con priorità più bassa rischia di non essere mai eseguito: __*Starvation*__.

Per quanto riguarda la prima problematica vi sono vari modi per assegnare una priorità. Le politiche di assegnamento si dividono in **interne al Sistema Operativo** o **Esterne al Sistema Operativo**.
Le prime possono riguardare: 

* La quantità di risorse che il processo utilizza.
* Il tempo trascorso da quando il processo è stato accettato nel sistema.
* Se il processo è o meno parte del sistema operativo.
* L'interazione che il processo ha con l'utente.
* etc.

Si è soliti imporre che i processi con maggior consumo di risorse abbiano una priorità più alta per far si che i processi liberi le risorse il più velocemente possibile, lasciandole libere per i prossimi processi in coda di attesa.

Le politiche esterne all'OS possono essere le più svariate, per esempio: 

* Importanza del processo.
* Proprietario del processo.
* Pagamento per l'utilizzo del computer (questo accadeva più nel passato che adesso).
* etc.

**N.B.** Per cambiare la priorità ad un processo su linux è possibile utilizzare il comando nice. Questo comando tuttavia non cambia la priorità totale del processo ma alza solo 1 dei vari parametri che la compongono di conseguenza **LA PRIORITÀ DI UN PROCESSO NON È MODIFICABILE DA PARTE DEL PROGRAMMATORE**.

Per quanto riguarda il problema di Starvation di un processo l'unico cosa che si può effettivamente fare è cambiare algoritmo e prendere un algoritmo che introduca l'aging (invecchiamento) la possibilità cioè di alzare la priorità di un processo che attende da molto tempo di essere eseguito.


#### HRRN - Highest Response Ratio Next

l'HRRN è un algoritmo di scheduling a priorità nel quale si assegna una priorità proporzionale al tempo di attesa del processo ed inversamente proporzionale alla lunghezza del burst di CPU. La formula che permete di calcolare tale priorità è la seguente:

<center>1 + (T<sub>attesa</sub> / Busrt<sub>CPU</sub>)</center>

**N.B.** Questo algoritmo si comporta come l'SJF, ma al contrario suo implementa un meccanismo di aging che è dato dall'inclusione del tempo di attesa nella formula.

Ecco un esempio di funzionamento dell'algoritmo:

![Esempio HRRN](res/HRRN_es.png)


#### RR - Round Robin

Questo è un algoritmo non basato su priorità, ma intrinsecamente preemptive. 
Il funzionamento è il seguente:

1. Ad ogni processo viene assegnato un quanto di tempo (tipicamente dai 10 ai 100 ms).
2. Allo scadere del quanto di tempo il processo viene prelazionato e messo nella ready queque, la quale viene trattata come una coda circolare.

Uno dei vantaggi principali di questo algoritmo è che ogni processo può attendere al massimo: <center>(n - 1) * q</center> dove:

* n &harr; numero dei processi.
* q &harr; quanto di tempo assegnato ad ogni processo.

Si può notare immediatamente la somiglianza di RR con FCFS infatti:
<center>q = &infin; &rarr; RR = FCFS</center>
Si noti anche che anche un quanto troppo piccolo presenta un problema poiché il context switch inizierebbe a predere un tempo del processo.
<center>q &lt;&lt; T<sub>context switch</sub> &rarr; utilizzo scarso della CPU </center>

Il problema della scelta del quanto è relativamente semplice da risolvere infatti basta scegliere q tale che circa l'80% dei processi termini il suo burst di CPU in meno di q così si limitano i tempi di attesa.

Per quanto riguarda le prestazioni possiamo paragonare RR a SJF nel seguente modo:

* T<sub>turnaround RR</sub> &ge; T<sub>turnaround SJF</sub>
* T<sub>risposta RR</sub> &le; T<sub>risposta SJF</sub>

#### Scheduling basato su code multilivello

Come abbiamo visto tutti questi algoritmi hanno vantaggi e svantaggi. Ovviamente in base ai processi che si stanno eseguendo, risulta essere più comodo utilizzarne uno piuttosto che un altro. Per esempio:

* Per i demoni (processi che non hanno interazione con l'utente) tornerà utile un algoritmo facile da implementare e che lasci lavorare il più possibile i processi. Si può dunque scegliere un FCFS.
* I processi in foreground che necessitano di una prolungata interazione con l'utente (hanno probabilmente un interfaccia grafica) avranno probabilmente dei burst di CPU molto brevi seguiti da alcuni burst di I/O molto lunghi (rispetto a quelli di CPU). Si può scegliere un algoritmo RR dato che probabilmente sarà possibile trovare un q abbastanza grande in modo che l'80% dei processi termini il burst di CPU prima della fine del quanto.

Per questo motivo nascono gli algoritmi di scheduling basati su code multilivello. Il concetto fondamentale di questi algoritmi è quello di prendere la Ready Queque e dividerla in più code gestite ognuna con un algoritmo diverso.

Ovviamente nonostante questo meccanismo sia molto comodo risulta molto complesso da implementare. 

Il problema principale che si trova implementando un sistema di questo tipo è lo **Scheduling tra le code**.
Per implementarlo ci sono due metodi principali:

1. Scheduling basato su time slice: ogni coda ottiene un quanto di tempo in cui schedula i suoi processi. Molto simile al RR.
2. Scheduling a priorità fissa: ad ogni coda viene assegnata una priorità e si eseguono prima tutti i processi della coda a priorità più alta e poi tutti i processi a priorità più bassa ( normalissimo scheduling a priorità ).

Anche in questo caso, con la prima soluzione, però si crea il problema della starvation. Basti immaginare di avere una coda a priorità molto alta molto popolata si rischia di non eseguire le code a priorità più bassa.

Anche in caso si scelga la seconda soluzione è possibile che un processo a priorità molto bassa in una coda a priorità molto bassa non venga mai eseguito. Per questo motivo sono state pensate le code con feedback in modo che permetta ad un processo di essere spostato da una coda ad un altra, implementando di fatto un meccanismo di aging.

Ecco un esempio di code multilivello:

![Code multilivello](res/es_multicode.png)

#### Scheduling Fair Share

Il Fair Share è un algoritmo che fa in modo di orientare lo scheduling non al singolo processo ma ad un gruppo di processi, che normalmente compongono un applicazione.

Per fare questo viene aggiunta una vase in cui si schedulano i gruppi di processi scegliendo quante risorse vengono allocate per ogni gruppo, ad esempio: Supponiamo di avere 3 applicazioni composte da n processi l'una e si decide che il primo gruppo deve avere il 25% del tempo di CPU il secondo il 35% ed il 3° il 40%. Effettuata questa fase si passa ad una seconda fase in cui si schedulano i processi appartenenti ad una singola applicazione. 

Esempio scheduler Fair Share

![Esempio Fair Share](res/es_fair-share.png)



